#!/usr/bin/env python3
"""
Test LLM-Enhanced Community Summarization
==========================================

Tests para verificar la nueva funcionalidad de Sprint 3:
LLM-enhanced community summarization con Google Gemini.
"""

import sys
import os
import time
from pathlib import Path

# Setup path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_llm_enhanced_summarization():
    """Test de LLM-enhanced community summarization"""
    
    print("ðŸ§  LLM-ENHANCED COMMUNITY SUMMARIZATION TEST")
    print("=" * 60)
    
    try:
        from rag_preprocessing.graph_rag.pipeline import GraphRAGPipeline, GraphRAGConfig
        
        # Test documento complejo para mejor evaluaciÃ³n LLM
        complex_document = """
        La colaboraciÃ³n entre Microsoft Research, OpenAI y Stanford University ha revolucionado 
        el campo del Graph RAG. El Dr. GarcÃ­a de la Universidad Complutense de Madrid lidera 
        un equipo de investigaciÃ³n que incluye a la Dra. Smith del MIT y al Prof. Johnson de 
        Google DeepMind. 
        
        Su trabajo se centra en algoritmos de detecciÃ³n de comunidades como Leiden y Louvain, 
        implementados usando NetworkX e igraph. Los sistemas de embeddings vectoriales utilizan 
        ChromaDB para desarrollo y Qdrant para producciÃ³n enterprise.
        
        El enfoque hÃ­brido combina bÃºsqueda local en comunidades especÃ­ficas con bÃºsqueda 
        global usando resÃºmenes jerÃ¡rquicos generados por LLMs como Gemini y GPT-4. Los 
        algoritmos de quantizaciÃ³n vectorial optimizan el almacenamiento y la velocidad 
        de bÃºsqueda en sistemas de gran escala.
        """
        
        print(f"ðŸ“„ DOCUMENTO DE PRUEBA:")
        print(f"   Longitud: {len(complex_document):,} caracteres")
        print(f"   Complejidad: Alta (entidades, relaciones, conceptos tÃ©cnicos)")
        
        # 1. Test con LLM activado
        print(f"\nðŸ§  TEST 1: LLM-ENHANCED SUMMARIZATION")
        config_llm = GraphRAGConfig(
            max_community_levels=3,
            min_community_size=2,
            use_llm_extraction=False,
            use_llm_summarization=True,  # Activar LLM
            community_algorithm="leiden"
        )
        
        pipeline_llm = GraphRAGPipeline(config_llm)
        
        start_time = time.time()
        result_llm = pipeline_llm.process_documents([complex_document])
        llm_time = time.time() - start_time
        
        print(f"   âœ… LLM Processing: {llm_time:.2f}s")
        print(f"   ðŸ“Š Entidades: {result_llm['entities']}")
        print(f"   ðŸ•¸ï¸  Comunidades: {result_llm['communities']}")
        print(f"   ðŸ“ Reportes: {result_llm['reports']}")
        print(f"   ðŸ§  LLM Enhanced: {result_llm['llm_enhanced_reports']}")
        print(f"   ðŸ”§ Modo: {result_llm['summarization_mode']}")
        
        # 2. Test con LLM desactivado (baseline)
        print(f"\nðŸ“Š TEST 2: STATISTICAL BASELINE")
        config_stat = GraphRAGConfig(
            max_community_levels=3,
            min_community_size=2,
            use_llm_extraction=False,
            use_llm_summarization=False,  # Desactivar LLM
            community_algorithm="leiden"
        )
        
        pipeline_stat = GraphRAGPipeline(config_stat)
        
        start_time = time.time()
        result_stat = pipeline_stat.process_documents([complex_document])
        stat_time = time.time() - start_time
        
        print(f"   âœ… Statistical Processing: {stat_time:.2f}s")
        print(f"   ðŸ“Š Entidades: {result_stat['entities']}")
        print(f"   ðŸ•¸ï¸  Comunidades: {result_stat['communities']}")
        print(f"   ðŸ“ Reportes: {result_stat['reports']}")
        print(f"   ðŸ§  LLM Enhanced: {result_stat['llm_enhanced_reports']}")
        print(f"   ðŸ”§ Modo: {result_stat['summarization_mode']}")
        
        # 3. ComparaciÃ³n de calidad
        print(f"\nðŸ“ˆ ANÃLISIS COMPARATIVO:")
        print(f"   â±ï¸  Tiempo LLM: {llm_time:.2f}s")
        print(f"   â±ï¸  Tiempo Statistical: {stat_time:.2f}s")
        print(f"   ðŸš€ Overhead LLM: {((llm_time - stat_time) / stat_time * 100):.1f}%")
        
        # 4. Verificar tÃ­tulos de comunidades
        print(f"\nðŸ·ï¸  COMPARACIÃ“N DE TÃTULOS DE COMUNIDADES:")
        
        # Test queries para verificar calidad
        test_queries = [
            "Â¿QuÃ© organizaciones colaboran en Graph RAG?",
            "Â¿QuiÃ©n lidera la investigaciÃ³n en Madrid?",
            "Â¿QuÃ© algoritmos se utilizan?"
        ]
        
        print(f"\nðŸ” TEST DE CONSULTAS:")
        for i, query in enumerate(test_queries, 1):
            print(f"\n   ðŸ“ CONSULTA {i}: {query}")
            
            # LLM-enhanced query
            llm_result = pipeline_llm.query(query, search_type="hybrid")
            print(f"      ðŸ§  LLM: {llm_result['answer'][:80]}...")
            print(f"         Confidence: {llm_result['confidence']:.2f}")
            
            # Statistical query
            stat_result = pipeline_stat.query(query, search_type="hybrid")
            print(f"      ðŸ“Š Stat: {stat_result['answer'][:80]}...")
            print(f"         Confidence: {stat_result['confidence']:.2f}")
        
        # 5. MÃ©tricas de Ã©xito
        success_metrics = {
            "llm_available": result_llm['llm_enhanced_reports'] > 0,
            "statistical_fallback": result_stat['llm_enhanced_reports'] == 0,
            "same_entities": result_llm['entities'] == result_stat['entities'],
            "same_communities": result_llm['communities'] == result_stat['communities'],
            "performance_acceptable": llm_time < stat_time * 10  # LLM puede ser hasta 10x mÃ¡s lento (realista)
        }
        
        print(f"\nâœ… MÃ‰TRICAS DE Ã‰XITO:")
        for metric, success in success_metrics.items():
            status = "âœ…" if success else "âŒ"
            print(f"   {status} {metric}: {success}")
        
        all_success = all(success_metrics.values())
        
        if all_success:
            print(f"\nðŸŽ‰ TODOS LOS TESTS EXITOSOS")
            print(f"âœ… LLM-Enhanced Community Summarization: FUNCIONANDO")
            return True
        else:
            print(f"\nâš ï¸  ALGUNOS TESTS FALLARON")
            return False
        
    except Exception as e:
        print(f"\nâŒ ERROR EN TEST LLM: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_api_configuration():
    """Test de configuraciÃ³n de API"""
    
    print(f"\nðŸ”§ TEST DE CONFIGURACIÃ“N API")
    print("=" * 40)
    
    try:
        import os
        from dotenv import load_dotenv
        
        # Cargar variables de entorno
        load_dotenv()
        
        google_api_key = os.getenv('GOOGLE_API_KEY')
        
        print(f"   ðŸ“ .env file: {'Found' if os.path.exists('.env') else 'Not found'}")
        print(f"   ðŸ—ï¸  Google API Key: {'Configured' if google_api_key and google_api_key != 'your_google_api_key_here' else 'Not configured'}")
        
        if google_api_key and google_api_key != 'your_google_api_key_here':
            print(f"   âœ… API configurada correctamente")
            print(f"   ðŸ’¡ LLM enhancement estarÃ¡ disponible")
            return True
        else:
            print(f"   âš ï¸  API no configurada")
            print(f"   ðŸ’¡ Sistema funcionarÃ¡ en modo estadÃ­stico")
            print(f"   ðŸ“‹ Para configurar: edita .env y aÃ±ade tu Google API key")
            return False
            
    except Exception as e:
        print(f"   âŒ Error checking API config: {e}")
        return False

def main():
    """FunciÃ³n principal"""
    
    print("ðŸ§ª LLM-ENHANCED GRAPH RAG - SPRINT 3 TEST")
    print("=" * 70)
    
    # Test configuraciÃ³n API
    api_configured = test_api_configuration()
    
    # Test principal
    llm_test_success = test_llm_enhanced_summarization()
    
    print(f"\n" + "=" * 70)
    print(f"ðŸ“Š RESUMEN DE TESTS:")
    print(f"   ðŸ”§ API Configuration: {'âœ… PASS' if api_configured else 'âš ï¸  FALLBACK'}")
    print(f"   ðŸ§  LLM Enhancement: {'âœ… PASS' if llm_test_success else 'âŒ FAIL'}")
    
    if llm_test_success:
        print(f"\nðŸŽ‰ SPRINT 3.1 LLM INTEGRATION: COMPLETADO")
        if api_configured:
            print(f"ðŸ§  LLM-enhanced community summarization funcionando")
        else:
            print(f"ðŸ“Š Statistical fallback funcionando correctamente")
    else:
        print(f"\nâŒ SPRINT 3.1: REQUIERE DEBUGGING")

if __name__ == "__main__":
    main() 